{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sustainable-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets.ice import IceWithProposals\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from models.basic_pga.utils import get_image_dicts, build_pos_tensors, build_rand_inds\n",
    "from models.basic_axial.basic_axial_parts import BlockAxial\n",
    "from models.basic_pga.basic_pga_parts import BlockPGA\n",
    "\n",
    "\n",
    "# def conv1x1(in_planes, out_planes, stride=1):\n",
    "#     return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "# class BlockPGA(nn.Module):\n",
    "#     def __init__(self, channels, embedding_dims, img_shape=(300, 300)):\n",
    "#         super(BlockPGA, self).__init__()\n",
    "#         self.channels = channels\n",
    "#         self.embedding_dims = embedding_dims\n",
    "#         self.embedding_dims_double = embedding_dims * 2\n",
    "#         self.img_shape = img_shape\n",
    "\n",
    "#         self.conv1 = conv1x1(self.channels, self.embedding_dims)\n",
    "#         self.bn1 = nn.BatchNorm2d(self.embedding_dims)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#         self.attn = PropAttention(dim=self.embedding_dims, heads=2, img_crop=img_shape[0])\n",
    "\n",
    "#         self.conv2 = conv1x1(self.embedding_dims_double, self.embedding_dims)\n",
    "#         self.bn2 = nn.BatchNorm2d(self.embedding_dims)\n",
    "\n",
    "#     def forward(self, x, obj_dict, bg_dict):\n",
    "#         # print(x.shape)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "\n",
    "#         x_attn = self.attn(x, obj_dict, bg_dict)\n",
    "#         x_attn = self.relu(x_attn)\n",
    "\n",
    "#         x = torch.cat((x_attn, x), dim=1)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class PropAttention(nn.Module):\n",
    "#     def __init__(self, dim, heads, img_crop, dim_heads=None):\n",
    "#         assert (dim % heads) == 0, 'hidden dimension must be divisible by number of heads'\n",
    "#         super().__init__()\n",
    "#         self.dim_heads = (dim // heads) if dim_heads is None else dim_heads\n",
    "\n",
    "#         inds = build_rand_inds(heads, img_crop)\n",
    "#         inds_tensor = torch.LongTensor(inds)\n",
    "#         self.rand_inds = nn.Parameter(inds_tensor, requires_grad=False)\n",
    "\n",
    "#         self.heads = heads\n",
    "#         self.img_crop = img_crop\n",
    "#         self.to_q = nn.Linear(self.dim_heads, self.dim_heads, bias=False)\n",
    "#         self.to_kv = nn.Linear(self.dim_heads, 2 * self.dim_heads, bias=False)\n",
    "#         self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "#     def construct(self, t, obj_dict, bg_dict, inds):\n",
    "#         img_flat = t.view(-1, self.img_crop ** 2)\n",
    "\n",
    "#         obj_vecs = []\n",
    "#         bg_vecs = []\n",
    "#         num_obj = self.img_crop // 2\n",
    "#         num_bg = self.img_crop - num_obj\n",
    "#         for i in range(num_obj):\n",
    "#             obj_vecs.append(build_pos_tensors(img_flat, obj_dict, inds[i]))\n",
    "#         for i in range(num_bg):\n",
    "#             bg_vecs.append(build_pos_tensors(img_flat, bg_dict, inds[i + num_obj]))\n",
    "\n",
    "#         obj_tensor = torch.stack(obj_vecs, dim=0)\n",
    "#         bg_tensor = torch.stack(bg_vecs, dim=0)\n",
    "\n",
    "#         all_tensor = torch.cat((obj_tensor, bg_tensor), dim=0)\n",
    "\n",
    "#         return all_tensor\n",
    "\n",
    "#     def destruct(self, t, img, obj_dict, bg_dict, inds):\n",
    "#         ts = torch.chunk(t, 2, dim=0)\n",
    "\n",
    "#         obj_tensor_tuple = torch.chunk(ts[0], ts[0].shape[0], dim=0)\n",
    "#         bg_tensor_tuple = torch.chunk(ts[1], ts[1].shape[0], dim=0)\n",
    "\n",
    "#         for i, vec in enumerate(obj_tensor_tuple):\n",
    "#             img = self.add_vec_to_tensor(img, inds[i], vec.squeeze(0).transpose(1, 0), obj_dict)\n",
    "#         for i, vec in enumerate(bg_tensor_tuple):\n",
    "#             img = self.add_vec_to_tensor(img, inds[i + len(obj_tensor_tuple)], vec.squeeze(0).transpose(1, 0), bg_dict)\n",
    "\n",
    "#         return img\n",
    "\n",
    "#     def add_vec_to_tensor(self, t, inds, vec, obj_dict):\n",
    "#         # for i, v in zip(inds, vec):\n",
    "#         for i, v in zip(inds.detach().cpu().numpy(), vec):\n",
    "#             obj_ind = obj_dict[i]\n",
    "#             x = obj_ind // self.img_crop\n",
    "#             y = obj_ind % self.img_crop\n",
    "#             t[:, :, x, y] = v\n",
    "#         return t\n",
    "\n",
    "#     def forward(self, x, obj_dict, bg_dict, kv=None):\n",
    "# #         x = x.clone().detach()\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         img_heads = torch.chunk(x.cpu(), self.heads, dim=1)\n",
    "#         head_list = []\n",
    "#         for inds, h in zip(self.rand_inds, img_heads):\n",
    "#             head_list.append(self.construct(h, obj_dict, bg_dict, inds))\n",
    "\n",
    "#         out = torch.cat(head_list, dim=0).view(self.heads * self.img_crop, self.img_crop, -1).cuda()\n",
    "#         print(f\"Constructing finished in {time.time() - start_time} seconds.\")\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         kv = out if kv is None else kv\n",
    "#         q, k, v = (self.to_q(out), *self.to_kv(kv).chunk(2, dim=-1))\n",
    "#         dots = torch.einsum('bie,bje->bij', q, k) * (self.dim_heads ** -0.5)\n",
    "#         dots = dots.softmax(dim=-1)\n",
    "#         out = torch.einsum('bij,bje->bie', dots, v)\n",
    "\n",
    "#         out = out.view(self.heads * self.img_crop, -1, self.img_crop).cpu()\n",
    "#         print(f\"Matrix math finished in {time.time() - start_time} seconds.\")\n",
    "        \n",
    "#         ts = torch.chunk(out, self.heads, dim=0)\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         new_img_list = []\n",
    "#         for t, img_head, inds in zip(ts, img_heads, self.rand_inds):\n",
    "#             new_img_list.append(self.destruct(t, img_head, obj_dict, bg_dict, inds))\n",
    "#         out_final = torch.cat(new_img_list, dim=1).permute(0, 2, 3, 1).contiguous().cuda()\n",
    "#         out_final = self.to_out(out_final)\n",
    "#         print(f\"Destructing finished in {time.time() - start_time} seconds.\")\n",
    "        \n",
    "#         return out_final.permute(0, 3, 1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lightweight-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "img_scale = .35\n",
    "img_crop = 220\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_set = IceWithProposals(os.path.join(data_dir, 'imgs'), os.path.join(data_dir, 'masks'),\n",
    "                  os.path.join(data_dir, 'txt_files'), os.path.join(data_dir, 'proposals/binary_250_16'),\n",
    "                               'train', img_scale, img_crop)\n",
    "train_loader = DataLoader(train_set, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nutritional-computer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading batch finished in 0.5951581001281738 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "batch = train_set[0]\n",
    "print(f\"Loading batch finished in {time.time() - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "blessed-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_dict, bg_dict = batch['obj_dict'], batch['bg_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cordless-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,3,img_crop,img_crop).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abstract-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = BlockPGA(3, 10, img_shape=(img_crop, img_crop)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "complex-pollution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing finished in 1.2000470161437988 seconds.\n",
      "Matrix math finished in 0.006580829620361328 seconds.\n",
      "Destructing finished in 2.502077341079712 seconds.\n",
      "Finished in 4.1572065353393555 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "out = block(x, obj_dict, bg_dict)\n",
    "print(f\"Finished in {time.time() - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-chapel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-zealand",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "structured-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = batch['prop']\n",
    "prop_flat = prop.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "critical-individual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True,  ..., True, True, True])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prop_flat==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "consecutive-hampton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16888 31512\n"
     ]
    }
   ],
   "source": [
    "inds_bg = (prop_flat == 0).nonzero(as_tuple=True)[0]\n",
    "inds_obj = (prop_flat == 1).nonzero(as_tuple=True)[0]\n",
    "print(len(inds_bg), len(inds_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "extensive-scotland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48400\n",
      "48400\n"
     ]
    }
   ],
   "source": [
    "extended_bg = torch.cat((inds_bg, inds_bg), dim=0)\n",
    "while len(extended_bg) < img_crop**2:\n",
    "    extended_bg = torch.cat((extended_bg, inds_bg), dim=0)\n",
    "extended_bg = extended_bg[:img_crop**2]\n",
    "print(len(extended_bg))\n",
    "\n",
    "extended_obj = torch.cat((inds_obj, inds_obj), dim=0)\n",
    "while len(extended_obj) < img_crop**2:\n",
    "    extended_obj = torch.cat((extended_obj, inds_obj), dim=0)\n",
    "extended_obj = extended_obj[:img_crop**2]\n",
    "print(len(extended_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "affected-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_dict = {i: x.item() for i,x in enumerate(extended_bg)}\n",
    "obj_dict = {i: x.item() for i,x in enumerate(extended_obj)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-nickel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-listing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-hearts",
   "metadata": {},
   "source": [
    "# On Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "further-cameroon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting dicts finished in 9.985198974609375 seconds.\n",
      "Constructing finished in 1.2408664226531982 seconds.\n",
      "Matrix math finished in 0.0014796257019042969 seconds.\n",
      "Destructing finished in 3.336125135421753 seconds.\n",
      "Finished in 15.011268854141235 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "out = block(x, prop)\n",
    "print(f\"Finished in {time.time() - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-wisdom",
   "metadata": {},
   "source": [
    "# On CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faced-operator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting dicts finished in 4.533999919891357 seconds.\n",
      "Constructing finished in 1.207212209701538 seconds.\n",
      "Matrix math finished in 0.06548714637756348 seconds.\n",
      "Destructing finished in 2.323352098464966 seconds.\n",
      "Finished in 8.481204509735107 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "out = block(x, prop)\n",
    "print(f\"Finished in {time.time() - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-newcastle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-thong",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "harmful-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randn(1, 20, img_crop, img_crop).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "greater-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_pga = BlockPGA(20, 20, img_shape=(img_crop, img_crop)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dedicated-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_axial = BlockAxial(10, 10, img_shape=(img_crop, img_crop)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = block_pga(test, obj_dict, bg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-mitchell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "complete-honolulu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight 100\n",
      "bn1.weight 10\n",
      "bn1.bias 10\n",
      "attn.to_q.weight 25\n",
      "attn.to_kv.weight 50\n",
      "attn.to_out.weight 100\n",
      "attn.to_out.bias 10\n",
      "conv2.weight 200\n",
      "bn2.weight 10\n",
      "bn2.bias 10\n",
      "525\n"
     ]
    }
   ],
   "source": [
    "params = 0\n",
    "for name, param in block_pga.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.numel())\n",
    "        params += param.numel()\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "severe-intelligence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight 100\n",
      "bn1.weight 10\n",
      "bn1.bias 10\n",
      "attn.axial_attentions.0.fn.to_q.weight 100\n",
      "attn.axial_attentions.0.fn.to_kv.weight 200\n",
      "attn.axial_attentions.0.fn.to_out.weight 100\n",
      "attn.axial_attentions.0.fn.to_out.bias 10\n",
      "attn.axial_attentions.1.fn.to_q.weight 100\n",
      "attn.axial_attentions.1.fn.to_kv.weight 200\n",
      "attn.axial_attentions.1.fn.to_out.weight 100\n",
      "attn.axial_attentions.1.fn.to_out.bias 10\n",
      "conv2.weight 200\n",
      "bn2.weight 10\n",
      "bn2.bias 10\n",
      "1160\n"
     ]
    }
   ],
   "source": [
    "params = 0\n",
    "for name, param in block_axial.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.numel())\n",
    "        params += param.numel()\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-karen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
