{"format": "torch", "nodes": [{"name": "block1", "id": 140173882050880, "class_name": "BlockAxial(\n  (conv1): Conv2d(3, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (attn): AxialAttention(\n    (axial_attentions): ModuleList(\n      (0): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=10, out_features=10, bias=False)\n          (to_kv): Linear(in_features=10, out_features=20, bias=False)\n          (to_out): Linear(in_features=10, out_features=10, bias=True)\n        )\n      )\n      (1): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=10, out_features=10, bias=False)\n          (to_kv): Linear(in_features=10, out_features=20, bias=False)\n          (to_out): Linear(in_features=10, out_features=10, bias=True)\n        )\n      )\n    )\n  )\n  (conv2): Conv2d(20, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)", "parameters": [["conv1.weight", [10, 3, 1, 1]], ["bn1.weight", [10]], ["bn1.bias", [10]], ["attn.axial_attentions.0.fn.to_q.weight", [10, 10]], ["attn.axial_attentions.0.fn.to_kv.weight", [20, 10]], ["attn.axial_attentions.0.fn.to_out.weight", [10, 10]], ["attn.axial_attentions.0.fn.to_out.bias", [10]], ["attn.axial_attentions.1.fn.to_q.weight", [10, 10]], ["attn.axial_attentions.1.fn.to_kv.weight", [20, 10]], ["attn.axial_attentions.1.fn.to_out.weight", [10, 10]], ["attn.axial_attentions.1.fn.to_out.bias", [10]], ["conv2.weight", [10, 20, 1, 1]], ["bn2.weight", [10]], ["bn2.bias", [10]]], "output_shape": [[1, 10, 220, 220]], "num_parameters": [30, 10, 10, 100, 200, 100, 10, 100, 200, 100, 10, 200, 10, 10]}, {"name": "block2", "id": 140173882007808, "class_name": "BlockAxial(\n  (conv1): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (attn): AxialAttention(\n    (axial_attentions): ModuleList(\n      (0): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=10, out_features=10, bias=False)\n          (to_kv): Linear(in_features=10, out_features=20, bias=False)\n          (to_out): Linear(in_features=10, out_features=10, bias=True)\n        )\n      )\n      (1): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=10, out_features=10, bias=False)\n          (to_kv): Linear(in_features=10, out_features=20, bias=False)\n          (to_out): Linear(in_features=10, out_features=10, bias=True)\n        )\n      )\n    )\n  )\n  (conv2): Conv2d(20, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)", "parameters": [["conv1.weight", [10, 10, 1, 1]], ["bn1.weight", [10]], ["bn1.bias", [10]], ["attn.axial_attentions.0.fn.to_q.weight", [10, 10]], ["attn.axial_attentions.0.fn.to_kv.weight", [20, 10]], ["attn.axial_attentions.0.fn.to_out.weight", [10, 10]], ["attn.axial_attentions.0.fn.to_out.bias", [10]], ["attn.axial_attentions.1.fn.to_q.weight", [10, 10]], ["attn.axial_attentions.1.fn.to_kv.weight", [20, 10]], ["attn.axial_attentions.1.fn.to_out.weight", [10, 10]], ["attn.axial_attentions.1.fn.to_out.bias", [10]], ["conv2.weight", [10, 20, 1, 1]], ["bn2.weight", [10]], ["bn2.bias", [10]]], "output_shape": [[1, 10, 220, 220]], "num_parameters": [100, 10, 10, 100, 200, 100, 10, 100, 200, 100, 10, 200, 10, 10]}, {"name": "block3", "id": 140173882050784, "class_name": "BlockAxial(\n  (conv1): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (attn): AxialAttention(\n    (axial_attentions): ModuleList(\n      (0): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=10, out_features=10, bias=False)\n          (to_kv): Linear(in_features=10, out_features=20, bias=False)\n          (to_out): Linear(in_features=10, out_features=10, bias=True)\n        )\n      )\n      (1): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=10, out_features=10, bias=False)\n          (to_kv): Linear(in_features=10, out_features=20, bias=False)\n          (to_out): Linear(in_features=10, out_features=10, bias=True)\n        )\n      )\n    )\n  )\n  (conv2): Conv2d(20, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)", "parameters": [["conv1.weight", [10, 10, 1, 1]], ["bn1.weight", [10]], ["bn1.bias", [10]], ["attn.axial_attentions.0.fn.to_q.weight", [10, 10]], ["attn.axial_attentions.0.fn.to_kv.weight", [20, 10]], ["attn.axial_attentions.0.fn.to_out.weight", [10, 10]], ["attn.axial_attentions.0.fn.to_out.bias", [10]], ["attn.axial_attentions.1.fn.to_q.weight", [10, 10]], ["attn.axial_attentions.1.fn.to_kv.weight", [20, 10]], ["attn.axial_attentions.1.fn.to_out.weight", [10, 10]], ["attn.axial_attentions.1.fn.to_out.bias", [10]], ["conv2.weight", [10, 20, 1, 1]], ["bn2.weight", [10]], ["bn2.bias", [10]]], "output_shape": [[1, 10, 220, 220]], "num_parameters": [100, 10, 10, 100, 200, 100, 10, 100, 200, 100, 10, 200, 10, 10]}, {"name": "block4", "id": 140173882049248, "class_name": "BlockAxial(\n  (conv1): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (attn): AxialAttention(\n    (axial_attentions): ModuleList(\n      (0): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=10, out_features=10, bias=False)\n          (to_kv): Linear(in_features=10, out_features=20, bias=False)\n          (to_out): Linear(in_features=10, out_features=10, bias=True)\n        )\n      )\n      (1): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=10, out_features=10, bias=False)\n          (to_kv): Linear(in_features=10, out_features=20, bias=False)\n          (to_out): Linear(in_features=10, out_features=10, bias=True)\n        )\n      )\n    )\n  )\n  (conv2): Conv2d(20, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)", "parameters": [["conv1.weight", [10, 10, 1, 1]], ["bn1.weight", [10]], ["bn1.bias", [10]], ["attn.axial_attentions.0.fn.to_q.weight", [10, 10]], ["attn.axial_attentions.0.fn.to_kv.weight", [20, 10]], ["attn.axial_attentions.0.fn.to_out.weight", [10, 10]], ["attn.axial_attentions.0.fn.to_out.bias", [10]], ["attn.axial_attentions.1.fn.to_q.weight", [10, 10]], ["attn.axial_attentions.1.fn.to_kv.weight", [20, 10]], ["attn.axial_attentions.1.fn.to_out.weight", [10, 10]], ["attn.axial_attentions.1.fn.to_out.bias", [10]], ["conv2.weight", [10, 20, 1, 1]], ["bn2.weight", [10]], ["bn2.bias", [10]]], "output_shape": [[1, 10, 220, 220]], "num_parameters": [100, 10, 10, 100, 200, 100, 10, 100, 200, 100, 10, 200, 10, 10]}, {"name": "outc", "id": 140173882051408, "class_name": "Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)", "parameters": [["weight", [3, 10, 1, 1]]], "output_shape": [[1, 3, 220, 220]], "num_parameters": [30]}], "edges": []}