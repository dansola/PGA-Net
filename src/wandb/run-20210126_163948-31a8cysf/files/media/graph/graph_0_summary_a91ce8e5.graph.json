{"format": "torch", "nodes": [{"name": "embed", "id": 140490974917776, "class_name": "Embed(\n  (embed): Linear(in_features=3, out_features=20, bias=False)\n  (pos): PositionalEncodingPermute2D(\n    (penc): PositionalEncoding2D()\n  )\n  (bn): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)", "parameters": [["embed.weight", [20, 3]], ["bn.weight", [20]], ["bn.bias", [20]]], "output_shape": [[1, 20, 320, 320]], "num_parameters": [60, 20, 20]}, {"name": "down1", "id": 140490974917968, "class_name": "AttentionDown(\n  (conv_down1): Sequential(\n    (0): Conv2d(20, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (conv_down2): Sequential(\n    (0): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (attn): AxialAttention(\n    (axial_attentions): ModuleList(\n      (0): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=10, out_features=10, bias=False)\n          (to_kv): Linear(in_features=10, out_features=20, bias=False)\n          (to_out): Linear(in_features=10, out_features=10, bias=True)\n        )\n      )\n      (1): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=10, out_features=10, bias=False)\n          (to_kv): Linear(in_features=10, out_features=20, bias=False)\n          (to_out): Linear(in_features=10, out_features=10, bias=True)\n        )\n      )\n    )\n  )\n  (conv_up): Conv2d(10, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (downsample): Sequential(\n    (0): Conv2d(20, 40, kernel_size=(1, 1), stride=(2, 2), bias=False)\n    (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)", "parameters": [["conv_down1.0.weight", [10, 20, 1, 1]], ["conv_down1.1.weight", [10]], ["conv_down1.1.bias", [10]], ["conv_down2.0.weight", [20, 20, 1, 1]], ["conv_down2.1.weight", [20]], ["conv_down2.1.bias", [20]], ["attn.axial_attentions.0.fn.to_q.weight", [10, 10]], ["attn.axial_attentions.0.fn.to_kv.weight", [20, 10]], ["attn.axial_attentions.0.fn.to_out.weight", [10, 10]], ["attn.axial_attentions.0.fn.to_out.bias", [10]], ["attn.axial_attentions.1.fn.to_q.weight", [10, 10]], ["attn.axial_attentions.1.fn.to_kv.weight", [20, 10]], ["attn.axial_attentions.1.fn.to_out.weight", [10, 10]], ["attn.axial_attentions.1.fn.to_out.bias", [10]], ["conv_up.weight", [20, 10, 1, 1]], ["bn.weight", [20]], ["bn.bias", [20]], ["downsample.0.weight", [40, 20, 1, 1]], ["downsample.1.weight", [40]], ["downsample.1.bias", [40]]], "output_shape": [[1, 40, 160, 160]], "num_parameters": [200, 10, 10, 400, 20, 20, 100, 200, 100, 10, 100, 200, 100, 10, 200, 20, 20, 800, 40, 40]}, {"name": "down2", "id": 140490952734944, "class_name": "AttentionDown(\n  (conv_down1): Sequential(\n    (0): Conv2d(40, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (conv_down2): Sequential(\n    (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (attn): AxialAttention(\n    (axial_attentions): ModuleList(\n      (0): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=20, out_features=20, bias=False)\n          (to_kv): Linear(in_features=20, out_features=40, bias=False)\n          (to_out): Linear(in_features=20, out_features=20, bias=True)\n        )\n      )\n      (1): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=20, out_features=20, bias=False)\n          (to_kv): Linear(in_features=20, out_features=40, bias=False)\n          (to_out): Linear(in_features=20, out_features=20, bias=True)\n        )\n      )\n    )\n  )\n  (conv_up): Conv2d(20, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (downsample): Sequential(\n    (0): Conv2d(40, 80, kernel_size=(1, 1), stride=(2, 2), bias=False)\n    (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)", "parameters": [["conv_down1.0.weight", [20, 40, 1, 1]], ["conv_down1.1.weight", [20]], ["conv_down1.1.bias", [20]], ["conv_down2.0.weight", [40, 40, 1, 1]], ["conv_down2.1.weight", [40]], ["conv_down2.1.bias", [40]], ["attn.axial_attentions.0.fn.to_q.weight", [20, 20]], ["attn.axial_attentions.0.fn.to_kv.weight", [40, 20]], ["attn.axial_attentions.0.fn.to_out.weight", [20, 20]], ["attn.axial_attentions.0.fn.to_out.bias", [20]], ["attn.axial_attentions.1.fn.to_q.weight", [20, 20]], ["attn.axial_attentions.1.fn.to_kv.weight", [40, 20]], ["attn.axial_attentions.1.fn.to_out.weight", [20, 20]], ["attn.axial_attentions.1.fn.to_out.bias", [20]], ["conv_up.weight", [40, 20, 1, 1]], ["bn.weight", [40]], ["bn.bias", [40]], ["downsample.0.weight", [80, 40, 1, 1]], ["downsample.1.weight", [80]], ["downsample.1.bias", [80]]], "output_shape": [[1, 80, 80, 80]], "num_parameters": [800, 20, 20, 1600, 40, 40, 400, 800, 400, 20, 400, 800, 400, 20, 800, 40, 40, 3200, 80, 80]}, {"name": "down3", "id": 140490952831232, "class_name": "AttentionDown(\n  (conv_down1): Sequential(\n    (0): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (conv_down2): Sequential(\n    (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (attn): AxialAttention(\n    (axial_attentions): ModuleList(\n      (0): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=40, out_features=40, bias=False)\n          (to_kv): Linear(in_features=40, out_features=80, bias=False)\n          (to_out): Linear(in_features=40, out_features=40, bias=True)\n        )\n      )\n      (1): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=40, out_features=40, bias=False)\n          (to_kv): Linear(in_features=40, out_features=80, bias=False)\n          (to_out): Linear(in_features=40, out_features=40, bias=True)\n        )\n      )\n    )\n  )\n  (conv_up): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (downsample): Sequential(\n    (0): Conv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n    (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)", "parameters": [["conv_down1.0.weight", [40, 80, 1, 1]], ["conv_down1.1.weight", [40]], ["conv_down1.1.bias", [40]], ["conv_down2.0.weight", [80, 80, 1, 1]], ["conv_down2.1.weight", [80]], ["conv_down2.1.bias", [80]], ["attn.axial_attentions.0.fn.to_q.weight", [40, 40]], ["attn.axial_attentions.0.fn.to_kv.weight", [80, 40]], ["attn.axial_attentions.0.fn.to_out.weight", [40, 40]], ["attn.axial_attentions.0.fn.to_out.bias", [40]], ["attn.axial_attentions.1.fn.to_q.weight", [40, 40]], ["attn.axial_attentions.1.fn.to_kv.weight", [80, 40]], ["attn.axial_attentions.1.fn.to_out.weight", [40, 40]], ["attn.axial_attentions.1.fn.to_out.bias", [40]], ["conv_up.weight", [80, 40, 1, 1]], ["bn.weight", [80]], ["bn.bias", [80]], ["downsample.0.weight", [160, 80, 1, 1]], ["downsample.1.weight", [160]], ["downsample.1.bias", [160]]], "output_shape": [[1, 160, 40, 40]], "num_parameters": [3200, 40, 40, 6400, 80, 80, 1600, 3200, 1600, 40, 1600, 3200, 1600, 40, 3200, 80, 80, 12800, 160, 160]}, {"name": "down4", "id": 140490952886496, "class_name": "AttentionDown(\n  (conv_down1): Sequential(\n    (0): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (conv_down2): Sequential(\n    (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (attn): AxialAttention(\n    (axial_attentions): ModuleList(\n      (0): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=80, out_features=80, bias=False)\n          (to_kv): Linear(in_features=80, out_features=160, bias=False)\n          (to_out): Linear(in_features=80, out_features=80, bias=True)\n        )\n      )\n      (1): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=80, out_features=80, bias=False)\n          (to_kv): Linear(in_features=80, out_features=160, bias=False)\n          (to_out): Linear(in_features=80, out_features=80, bias=True)\n        )\n      )\n    )\n  )\n  (conv_up): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (downsample): Sequential(\n    (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n    (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)", "parameters": [["conv_down1.0.weight", [80, 160, 1, 1]], ["conv_down1.1.weight", [80]], ["conv_down1.1.bias", [80]], ["conv_down2.0.weight", [160, 160, 1, 1]], ["conv_down2.1.weight", [160]], ["conv_down2.1.bias", [160]], ["attn.axial_attentions.0.fn.to_q.weight", [80, 80]], ["attn.axial_attentions.0.fn.to_kv.weight", [160, 80]], ["attn.axial_attentions.0.fn.to_out.weight", [80, 80]], ["attn.axial_attentions.0.fn.to_out.bias", [80]], ["attn.axial_attentions.1.fn.to_q.weight", [80, 80]], ["attn.axial_attentions.1.fn.to_kv.weight", [160, 80]], ["attn.axial_attentions.1.fn.to_out.weight", [80, 80]], ["attn.axial_attentions.1.fn.to_out.bias", [80]], ["conv_up.weight", [160, 80, 1, 1]], ["bn.weight", [160]], ["bn.bias", [160]], ["downsample.0.weight", [320, 160, 1, 1]], ["downsample.1.weight", [320]], ["downsample.1.bias", [320]]], "output_shape": [[1, 320, 20, 20]], "num_parameters": [12800, 80, 80, 25600, 160, 160, 6400, 12800, 6400, 80, 6400, 12800, 6400, 80, 12800, 160, 160, 51200, 320, 320]}, {"name": "up1", "id": 140490952887888, "class_name": "AttentionUp(\n  (conv_down1): Sequential(\n    (0): Conv2d(320, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Upsample(scale_factor=2.0, mode=bilinear)\n  )\n  (conv_down2): Sequential(\n    (0): Conv2d(320, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Upsample(scale_factor=2.0, mode=bilinear)\n  )\n  (attn): AxialAttention(\n    (axial_attentions): ModuleList(\n      (0): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=160, out_features=160, bias=False)\n          (to_kv): Linear(in_features=160, out_features=320, bias=False)\n          (to_out): Linear(in_features=160, out_features=160, bias=True)\n        )\n      )\n      (1): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=160, out_features=160, bias=False)\n          (to_kv): Linear(in_features=160, out_features=320, bias=False)\n          (to_out): Linear(in_features=160, out_features=160, bias=True)\n        )\n      )\n    )\n  )\n  (conv_up): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (conv_up_encoder): Conv2d(320, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n)", "parameters": [["conv_down1.0.weight", [160, 320, 1, 1]], ["conv_down1.1.weight", [160]], ["conv_down1.1.bias", [160]], ["conv_down2.0.weight", [160, 320, 1, 1]], ["conv_down2.1.weight", [160]], ["conv_down2.1.bias", [160]], ["attn.axial_attentions.0.fn.to_q.weight", [160, 160]], ["attn.axial_attentions.0.fn.to_kv.weight", [320, 160]], ["attn.axial_attentions.0.fn.to_out.weight", [160, 160]], ["attn.axial_attentions.0.fn.to_out.bias", [160]], ["attn.axial_attentions.1.fn.to_q.weight", [160, 160]], ["attn.axial_attentions.1.fn.to_kv.weight", [320, 160]], ["attn.axial_attentions.1.fn.to_out.weight", [160, 160]], ["attn.axial_attentions.1.fn.to_out.bias", [160]], ["conv_up.weight", [160, 160, 1, 1]], ["conv_up_encoder.weight", [160, 320, 1, 1]], ["bn.weight", [160]], ["bn.bias", [160]]], "output_shape": [[1, 160, 40, 40]], "num_parameters": [51200, 160, 160, 51200, 160, 160, 25600, 51200, 25600, 160, 25600, 51200, 25600, 160, 25600, 51200, 160, 160]}, {"name": "up2", "id": 140490935186432, "class_name": "AttentionUp(\n  (conv_down1): Sequential(\n    (0): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Upsample(scale_factor=2.0, mode=bilinear)\n  )\n  (conv_down2): Sequential(\n    (0): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Upsample(scale_factor=2.0, mode=bilinear)\n  )\n  (attn): AxialAttention(\n    (axial_attentions): ModuleList(\n      (0): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=80, out_features=80, bias=False)\n          (to_kv): Linear(in_features=80, out_features=160, bias=False)\n          (to_out): Linear(in_features=80, out_features=80, bias=True)\n        )\n      )\n      (1): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=80, out_features=80, bias=False)\n          (to_kv): Linear(in_features=80, out_features=160, bias=False)\n          (to_out): Linear(in_features=80, out_features=80, bias=True)\n        )\n      )\n    )\n  )\n  (conv_up): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (conv_up_encoder): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n)", "parameters": [["conv_down1.0.weight", [80, 160, 1, 1]], ["conv_down1.1.weight", [80]], ["conv_down1.1.bias", [80]], ["conv_down2.0.weight", [80, 160, 1, 1]], ["conv_down2.1.weight", [80]], ["conv_down2.1.bias", [80]], ["attn.axial_attentions.0.fn.to_q.weight", [80, 80]], ["attn.axial_attentions.0.fn.to_kv.weight", [160, 80]], ["attn.axial_attentions.0.fn.to_out.weight", [80, 80]], ["attn.axial_attentions.0.fn.to_out.bias", [80]], ["attn.axial_attentions.1.fn.to_q.weight", [80, 80]], ["attn.axial_attentions.1.fn.to_kv.weight", [160, 80]], ["attn.axial_attentions.1.fn.to_out.weight", [80, 80]], ["attn.axial_attentions.1.fn.to_out.bias", [80]], ["conv_up.weight", [80, 80, 1, 1]], ["conv_up_encoder.weight", [80, 160, 1, 1]], ["bn.weight", [80]], ["bn.bias", [80]]], "output_shape": [[1, 80, 80, 80]], "num_parameters": [12800, 80, 80, 12800, 80, 80, 6400, 12800, 6400, 80, 6400, 12800, 6400, 80, 6400, 12800, 80, 80]}, {"name": "up3", "id": 140490935187824, "class_name": "AttentionUp(\n  (conv_down1): Sequential(\n    (0): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Upsample(scale_factor=2.0, mode=bilinear)\n  )\n  (conv_down2): Sequential(\n    (0): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Upsample(scale_factor=2.0, mode=bilinear)\n  )\n  (attn): AxialAttention(\n    (axial_attentions): ModuleList(\n      (0): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=40, out_features=40, bias=False)\n          (to_kv): Linear(in_features=40, out_features=80, bias=False)\n          (to_out): Linear(in_features=40, out_features=40, bias=True)\n        )\n      )\n      (1): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=40, out_features=40, bias=False)\n          (to_kv): Linear(in_features=40, out_features=80, bias=False)\n          (to_out): Linear(in_features=40, out_features=40, bias=True)\n        )\n      )\n    )\n  )\n  (conv_up): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (conv_up_encoder): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n)", "parameters": [["conv_down1.0.weight", [40, 80, 1, 1]], ["conv_down1.1.weight", [40]], ["conv_down1.1.bias", [40]], ["conv_down2.0.weight", [40, 80, 1, 1]], ["conv_down2.1.weight", [40]], ["conv_down2.1.bias", [40]], ["attn.axial_attentions.0.fn.to_q.weight", [40, 40]], ["attn.axial_attentions.0.fn.to_kv.weight", [80, 40]], ["attn.axial_attentions.0.fn.to_out.weight", [40, 40]], ["attn.axial_attentions.0.fn.to_out.bias", [40]], ["attn.axial_attentions.1.fn.to_q.weight", [40, 40]], ["attn.axial_attentions.1.fn.to_kv.weight", [80, 40]], ["attn.axial_attentions.1.fn.to_out.weight", [40, 40]], ["attn.axial_attentions.1.fn.to_out.bias", [40]], ["conv_up.weight", [40, 40, 1, 1]], ["conv_up_encoder.weight", [40, 80, 1, 1]], ["bn.weight", [40]], ["bn.bias", [40]]], "output_shape": [[1, 40, 160, 160]], "num_parameters": [3200, 40, 40, 3200, 40, 40, 1600, 3200, 1600, 40, 1600, 3200, 1600, 40, 1600, 3200, 40, 40]}, {"name": "up4", "id": 140490935189216, "class_name": "AttentionUp(\n  (conv_down1): Sequential(\n    (0): Conv2d(40, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Upsample(scale_factor=2.0, mode=bilinear)\n  )\n  (conv_down2): Sequential(\n    (0): Conv2d(40, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Upsample(scale_factor=2.0, mode=bilinear)\n  )\n  (attn): AxialAttention(\n    (axial_attentions): ModuleList(\n      (0): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=20, out_features=20, bias=False)\n          (to_kv): Linear(in_features=20, out_features=40, bias=False)\n          (to_out): Linear(in_features=20, out_features=20, bias=True)\n        )\n      )\n      (1): PermuteToFrom(\n        (fn): SelfAttention(\n          (to_q): Linear(in_features=20, out_features=20, bias=False)\n          (to_kv): Linear(in_features=20, out_features=40, bias=False)\n          (to_out): Linear(in_features=20, out_features=20, bias=True)\n        )\n      )\n    )\n  )\n  (conv_up): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (conv_up_encoder): Conv2d(40, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n)", "parameters": [["conv_down1.0.weight", [20, 40, 1, 1]], ["conv_down1.1.weight", [20]], ["conv_down1.1.bias", [20]], ["conv_down2.0.weight", [20, 40, 1, 1]], ["conv_down2.1.weight", [20]], ["conv_down2.1.bias", [20]], ["attn.axial_attentions.0.fn.to_q.weight", [20, 20]], ["attn.axial_attentions.0.fn.to_kv.weight", [40, 20]], ["attn.axial_attentions.0.fn.to_out.weight", [20, 20]], ["attn.axial_attentions.0.fn.to_out.bias", [20]], ["attn.axial_attentions.1.fn.to_q.weight", [20, 20]], ["attn.axial_attentions.1.fn.to_kv.weight", [40, 20]], ["attn.axial_attentions.1.fn.to_out.weight", [20, 20]], ["attn.axial_attentions.1.fn.to_out.bias", [20]], ["conv_up.weight", [20, 20, 1, 1]], ["conv_up_encoder.weight", [20, 40, 1, 1]], ["bn.weight", [20]], ["bn.bias", [20]]], "output_shape": [[1, 20, 320, 320]], "num_parameters": [800, 20, 20, 800, 20, 20, 400, 800, 400, 20, 400, 800, 400, 20, 400, 800, 20, 20]}, {"name": "outc", "id": 140490934924432, "class_name": "Conv2d(20, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)", "parameters": [["weight", [3, 20, 1, 1]]], "output_shape": [[1, 3, 320, 320]], "num_parameters": [60]}], "edges": []}